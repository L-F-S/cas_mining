{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1><center>Pam Search test</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: right\">Lorenzo federico Signorini, 23/09/2019</div>\n",
    "\n",
    "<div style=\"text-align: right\">Universit√† degli studi di Trento</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23/09/2019\n",
    "Let' s try to  extract spacer sequences and lign them to all known contigs for the following Cas9s:\n",
    ">8774__OBKKDPJE_00146__BritoIL_2016__M2.46.ST__bin.47;length1073;#sequences1\n",
    ">8774__IBLHLBHI_00561__BritoIL_2016__W2.19.ST__bin.40;length1073;#sequences1\n",
    "\n",
    ">8774__LLPOPKBG_01052__QinN_2014__HD-66__bin.16;length1073;#sequences1\n",
    "\n",
    ">15299__PAOEJHHN_00966__FengQ_2015__SID531128__bin.48;15299__MOACNHFO_01282__XieH_2016__YSZC12003_36674__bin.32;length1074;#sequences2\n",
    ">15299__LPEAHEHG_01071__ChengpingW_2017__AS43raw__bin.51;length1074;#sequences1\n",
    "\n",
    ">4329__CJFLGLGO_01367__Obregon-TitoAJ_2015__SM40__bin.4;length1079;#sequences1\n",
    "\n",
    ">4329__GNIJMFKL_00179__Obregon-TitoAJ_2015__HCO64__bin.33;length1079;#sequences1\n",
    "\n",
    ">4329__OOELONHJ_00933__Obregon-TitoAJ_2015__SM01__bin.19;length1079;#sequences1\n",
    "\n",
    "Each one of these is actually the centroid of a cluster of sequences, moreover, some of these actually are identical sequences from different genomes. It would be nice to see whether their CRISPR arrays are similar, too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First created Thu Jul 11 09:51:15 CEST 2019\n",
    "# Made by L-F-S\n",
    "# At the University Of Trento, Italy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "# for clustal alignments\n",
    "from Bio import AlignIO\n",
    "from Bio.Align.Applications import ClustalwCommandline\n",
    "from Bio.Align import AlignInfo\n",
    "# to make logos\n",
    "import logomaker as lm\n",
    "\n",
    "sys.path.insert(0, '/home/lorenzo.signorini/cas_mining/utils/')\n",
    "import filename_discrepancies\n",
    "import originalpath\n",
    "sys.path.insert(0, '/home/lorenzo.signorini/cas_mining/9casminer_v2/')\n",
    "import locus\n",
    "\n",
    "\n",
    "feature=\"Cas9\" #WARNING!!! CHANGE THIS!!\n",
    "datadir=\"/shares/CIBIO-Storage/CM/news/users/lorenzo.signorini/8pamsearch/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Seq ID', 'Seq Description', 'Seq', 'Contig', 'Genome Name', 'Study',\n",
       "       'Sample Name', 'SGB ID', 'Pos', 'pilercr_CRISPR', 'minced_CRISPR',\n",
       "       'prokka_cas1', 'prokka_cas2', 'uSGB', 'Level of estimated taxonomy',\n",
       "       'Estimated taxonomy', 'prokka_cas9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "cas_dataset=pd.read_csv(\"/shares/CIBIO-Storage/CM/news/users/lorenzo.signorini/5caslocitable/known_\"+feature+\"_variants_table.csv\", index_col=0)\n",
    "cas_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da runnare tramite python script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ID:  LEAPHKCP_01380\n",
      "Contig: NODE_1601_length_16622_cov_6.41836\n",
      "Dataset: BritoIL_2016\n",
      "Genome name BritoIL_2016__M2.36.ST__bin.48\n",
      "Sample name: M2.36.ST\n",
      "SGB ID: 9311\n",
      "Cas9 length: 1065\n",
      "----------------------------------------------------------------------\n",
      "old epasolli genomename: BritoIL_2016__M2.36.ST__bin.48 BritoIL_2016\n",
      "TESTING, CHANGE DATADIR IF RUNNING PER DAVVERO\n"
     ]
    }
   ],
   "source": [
    "seqid =\"LEAPHKCP_01380\"   # MANUAL INPUT\n",
    "\n",
    "dataset=cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"Study\"].iloc[0]\n",
    "genomename=cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"Genome Name\"].iloc[0]\n",
    "SGB=cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"SGB ID\"].iloc[0]\n",
    "contigname=cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"Contig\"].iloc[0]\n",
    "samplename=cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"Sample Name\"].iloc[0]\n",
    "#\n",
    "full_genome_full_path, annotation_folder=originalpath.print_path(genomename)\n",
    "#\n",
    "old_genomename, old_dataset=filename_discrepancies.get_originalsamplename_froms3name_of_genome(genomename,dataset)\n",
    "old_path=\"/shares/CIBIO-Storage/CM/scratch/users/e.pasolli/projects/binning/genomes_comp50_cont05/\"+old_dataset+\"/\"\n",
    "\n",
    "#\n",
    "print(\"Sequence ID: \", seqid)\n",
    "print(\"Contig:\", contigname)\n",
    "print(\"Dataset:\",dataset)\n",
    "print(\"Genome name\", genomename)\n",
    "print(\"Sample name:\", samplename)\n",
    "print(\"SGB ID:\", SGB)\n",
    "print(\"Cas9 length:\", len(cas_dataset[cas_dataset[\"Seq ID\"]==seqid][\"Seq\"].iloc[0]))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"old epasolli genomename:\", old_genomename, old_dataset )\n",
    "\n",
    "print(\"TESTING, CHANGE DATADIR IF RUNNING PER DAVVERO\")\n",
    "outdir=datadir+\"out/test/\"#+seqid+\"__\"+genomename+\"/\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacers ['TTGTTTTGGCGGCGTTGAACGCGTCGGCGA', 'ATTAAGTGGGTTGATTATTACTGATATTTG', 'ATACGGTTGAAGTCATTGCCGCAAGTTCAG', 'CTTTGATTTTCATTTGTTTTACTCCTTTTC', 'AACATGGACGGTTACCTGTTGAAGCGTATC', 'CGGGGTCCCTTACGGGGCCCCTTGGTGTAG', 'GCTTTCAAAGACGCTATTTCCTTGTCTTTT', 'CGCGGAAACGGGCGTATTCGGAAAGGTATC', 'TGTAAATCAAATCGTACGCACGAAACGGAA', 'CTTTCGCAGTTTCGTTATCCACTAAAGCTA', 'TGGCGGGCTTGGGTCTGATTTTGTGGTATA', 'AAACGGGCGCGCAGTATGATGCGGTTGTG', 'CCCGCGACAATCGCACCGCCTGCGTTCGCG', 'AGAACAAACATATTATCCACGCAAGAAAGGA', 'AATGGCGCTCGACTTTGTTTCGAAGTTTAG', 'TCGTTAGAGTTGCGCGGGAGGGTATCAACG', 'TTTTTATAAGTTAGCTTTACCCTGCTAAAG', 'CGACGCGCCCTTGGCATCGATTGAAAGACG', 'CTGTACACGAACGGCGACGGTGCGGGCTTC', 'GTCCTTGCCGTGCTTATAAACACAATAGTC', 'TCCGCAGTCCTGCGGTTGTTTCGGCGACCG']\n"
     ]
    }
   ],
   "source": [
    "# 1 get CRISPR spacer sequence\n",
    "# get CRISPR spacer and repeat sequence\n",
    "cr=locus.CRISPRarray(feature=feature, contigname=contigname, genomename=genomename, datasetname=dataset)\n",
    "cr.get_CRISPR_array()\n",
    "spacers=cr.spacers\n",
    "#repeats=cr.repeats\n",
    "#repeat_start_pos=cr.repstartpos\n",
    "#unique_repeats=np.unique(repeats)\n",
    "#unique_spacers=np.unique(spacers)\n",
    "#todo I think we only care about spacers, no repeats, so all lines above commented out can be cancelled\n",
    "print(\"Spacers\", spacers )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. build temporary query file for blastn search, with spacers\n",
    "blast_folder=outdir\n",
    "os.chdir(blast_folder)\n",
    "tempfile=open(outdir+\"temp_spacer_seq\", \"w\")\n",
    "tempfile.close()\n",
    "tempfile=open(outdir+\"temp_spacer_seq\", \"a\")\n",
    "for n, spacer in enumerate(spacers):\n",
    "    tempfile.write(\">spacer\"+str(n+1)+\"|\"+contigname+\"|\"+genomename+\"|\"+seqid+\"\\n\"+spacer+\"\\n\") \n",
    "tempfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAST. Must parallelize across ALL samples in the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fare un log, per ogni query (contig di partenza), che printi quell'output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runan la cell qua sotto, e capisci la seq downstream come indicizzarla x averne 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn of query file agianst all contigs\n",
      "/home/lorenzo.signorini/ncbi-blast-2.10.0+/bin/blastn -out /shares/CIBIO-Storage/CM/news/users/lorenzo.signorini/8pamsearch/out/test/LEAPHKCP_01380_vs_VogtmannE_2016__MMRS68910755ST-27-0-0.contigs_filtered.fasta.blastout -outfmt \"6  qseqid sseqid pident qlen length mismatch gapopen qseq sseq sstart send    evalue sstrand\" -query temp_spacer_seq -db /shares/CIBIO-Storage/CM/news/users/lorenzo.signorini/8pamsearch/VogtmannE_2016/VogtmannE_2016__MMRS68910755ST-27-0-0.contigs_filtered.fasta -evalue 0.001 -word_size 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f03099f6278>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Blast query file against db\n",
    "\n",
    "# BackhedF_2015__SID130_M.contigs_filtered.fasta,    questo nel contig  NODE_16046_length_1718_cov_1.75827 non funge la seq upstream. esempio\n",
    "# mentre BackhedF_2015__SID26_12M_megahit.contigs_filtered.fasta ha la seq DOWNStream corta (come accate ad altre), nel contig NODE_6634_length_2333_cov_7.0000_ID_14245\n",
    "# altra downstream corta, x fare un dataset diverso: LawrenceA_2015__LD-Run2-11_megahit.contigs_filtered     NODE_1465_length_4048_cov_9.0000_ID_34458\n",
    "print(\"Running blastn of query file agianst all contigs\")\n",
    "target_sample_file=\"VogtmannE_2016__MMRS68910755ST-27-0-0.contigs_filtered.fasta\"#old_dataset+\"__\"+old_genomename.split(\"__\")[1] #\n",
    "dbfile=datadir+\"VogtmannE_2016/\"+target_sample_file # dataset+\"/\"+target_sample_file+\".contigs_filtered.fasta\" #\n",
    "blastoutfile=outdir+seqid+\"_vs_\"+target_sample_file+\".blastout\"\n",
    "blastn_command = \"/home/lorenzo.signorini/ncbi-blast-2.10.0+/bin/blastn -out \"+blastoutfile+\\\n",
    "    \" -outfmt \\\"6  qseqid sseqid pident qlen length mismatch gapopen qseq sseq sstart send\\\n",
    "    evalue sstrand\\\" -query temp_spacer_seq -db \"+dbfile+\" -evalue 0.001 -word_size 12\"\n",
    "print(blastn_command)\n",
    "subprocess.Popen(blastn_command, shell=True)\n",
    "\n",
    "#per dopo pranzo, fai uno script che fa una lista di nomi di tti icontigs filtered,riguarda se i contigs_filtered siano solamente i non assemblati (ma non credo),e se c'√® mdo nel file vecchio di vedere se le hit che avevi venivano da contigs non allineati. la risposta √®: lo facevi con chewck if unbinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query seq\tMatched_contig\t% identity\tquery length\talignment length\t# mismatches\t# gap opens \t query sequence\taligned part of subject sequence\tstart in subject\tend in subject\texpect. value\tmatched contig strand\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "spacer4|NODE_1601_length_16622_cov_6.41836|BritoIL_2016__M2.36.ST__bin.48|LEAPHKCP_01380\tNODE_16721_length_1719_cov_2.81731\t96.296\t30\t27\t0\t1\tTGATTTTCATTTGTTTTACTCCTTTTC\tTGATTTTCATTTGTTTT-CTCCTTTTC\t1092\t1067\t1.18e-04\tminus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 blast output parsing\n",
    "# 4.1 Print all output\n",
    "protospacers_of={} # save all putative protospacers, for every contig\n",
    "f=open(blastoutfile)\n",
    "lines=f.readlines()\n",
    "print(\"query seq\\tMatched_contig\\t% identity\\tquery length\\talignment length\\t# mismatches\\t# gap opens \\t query sequence\\taligned part of subject sequence\\tstart in subject\\tend in subject\\texpect. value\\tmatched contig strand\\n----------------------------------------------------------------------------------------------------------------------\")\n",
    "for line in lines:\n",
    "    target_cont=line.strip(\"\\n\").split()[1]  # contig name of target (putative protospacer)\n",
    "\n",
    "    if target_cont!=contigname: #filter out putative protospacers coming from same contig\n",
    "        print(line)\n",
    "        if not target_cont in protospacers_of.keys():\n",
    "            protospacers_of[target_cont]=[]\n",
    "        # add putative protospacer sequence to dictionary\n",
    "        protospacers_of[target_cont].append((int(line.strip(\"\\n\").split()[-4]),int(line.strip(\"\\n\").split()[-3]),line.strip(\"\\n\").split()[-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZvvvvVXTTTTTTTTTRGgggg\n",
      "full sequence ccccCYAAAAAAAAAXBbbbbZ\n",
      "extract sequence using blast start and end indexes\n",
      "my protospacer: l[start-1:end] YAAAAAAAAAX\n",
      "last letter of protospacer: l[end-1]] X\n",
      "First letter of downstream flanking region: l[end] B\n",
      "ALL downstream sequence l[end:] BbbbbZ\n",
      "iI know..\n",
      "first (or last) letter of upstream flanking: l[start] A\n",
      "ALL upstream sequence l[:start-1] ccccC\n",
      "I know..\n",
      "6 6\n",
      "XccccC 6\n"
     ]
    }
   ],
   "source": [
    "#voglio 6 nt a valle (downstream) di protospacer AAAAAAAAAX: ne ho solo 5 (BBBBB)\n",
    "# to understand how string slicing works and hos slicing is different than indexing with strings\n",
    "#and it behaves differently than lists\n",
    "l=\"ccccCYAAAAAAAAAXBbbbbZ\"\n",
    "lrevcomp=Seq(l).reverse_complement()\n",
    "print(lrevcomp)\n",
    "start=6 #output da blast\n",
    "end=16 # output da blast\n",
    "#lo voglio lungo 50\n",
    "print(\"full sequence\",l)\n",
    "print(\"extract sequence using blast start and end indexes\")\n",
    "print(\"my protospacer: l[start-1:end]\", l[start-1:end])\n",
    "print(\"last letter of protospacer: l[end-1]]\", l[end-1])\n",
    "print(\"First letter of downstream flanking region: l[end]\",l[end])\n",
    "print(\"ALL downstream sequence l[end:]\",l[end:])\n",
    "print(\"iI know..\")\n",
    "print(\"first (or last) letter of upstream flanking: l[start]\", l[start])\n",
    "print(\"ALL upstream sequence l[:start-1]\",l[:start-1])\n",
    "\n",
    "\n",
    "print(\"I know..\")\n",
    "last_pos=len(l)\n",
    "flanking_length=6\n",
    "print(start,flanking_length)\n",
    "if start>=(flanking_length+1):\n",
    "    upstream_start=start-(flanking_length+1) \n",
    "    print(upstream_start)\n",
    "    added_nucleotides=\"\"\n",
    "else:\n",
    "    upstream_start=0\n",
    "    added_nucleotides=\"X\"*((flanking_length+1)-start)\n",
    "upseq=added_nucleotides+l[upstream_start:start-1] #immedia\n",
    "print(upseq,len(upseq))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting flanking sequences. There are 1 contigs withputative protospacers.\n",
      "Searching for flanking sequences of protospacer from contig:\n",
      " NODE_16721_length_1719_cov_2.81731 \n",
      "\n",
      "checking if binned\n",
      "Match on minus, taking reverse complement\n",
      "Is extracted putative protospacer equal to Blasted? False\n",
      "original match: TGATTTTCATTTGTTTT-CTCCTTTTC\n",
      "start: 1067 end: 1092\n",
      "protospacer extracted: TGATTTTCATTTGTTTTCTCCTTTTC\n",
      "upstream sequence\n",
      "TEST: should end with TACC\n",
      "TEST: should end with TACC\n",
      "downstream sequence\n",
      "up TAAACTTACTCCCATTGTTACTGCCAGCATGCCTCCCACTAATTTGTTTC 50\n",
      "down: ATTTTTTCATGCGGACGCATCCTTCCAAAGCTCTTCCTTTACTGCCGAAA 50\n",
      "{'NODE_16721_length_1719_cov_2.81731': [(Seq('TAAACTTACTCCCATTGTTACTGCCAGCATGCCTCCCACTAATTTGTTTC', SingleLetterAlphabet()), Seq('ATTTTTTCATGCGGACGCATCCTTCCAAAGCTCTTCCTTTACTGCCGAAA', SingleLetterAlphabet()))]}\n"
     ]
    }
   ],
   "source": [
    "### 2 putative viral contigs extraction\n",
    "#TODO note to self: THERE IS ONLY ONE contigname now, but se ne faccio\n",
    "# di piu, will need to map target contigs to query contig in another dictionary\n",
    "# extract thje +50, -50 sequence flanking protospacer, for every contig (ASSUMING 1 contig = 1 protospacer) TODOable\n",
    "# for the particular locus studied, for every contig of putative protospacers\n",
    "print(\"Extracting flanking sequences. There are\", len(list(protospacers_of.keys())), \"contigs with\\\n",
    "putative protospacers.\")\n",
    "flanking_sequences_of_putative_protospacers_of={}\n",
    "f=open(outdir+\"flanking_sequences_of_putative_protospacers_\"+target_sample_file,\"w\")\n",
    "f=open(outdir+\"flanking_sequences_of_putative_protospacers_\"+target_sample_file,\"a\")\n",
    "for target_contig in protospacers_of.keys():\n",
    "    print(\"Searching for flanking sequences of protospacer from contig:\\n\",target_contig,\"\\n\")\n",
    "\n",
    "    # check if binned\n",
    "    print(\"checking if binned\")\n",
    "    target_bin='0'\n",
    "    os.chdir(old_path)\n",
    "    for file in os.listdir():\n",
    "        if file.startswith(target_sample_file):\n",
    "            for record in SeqIO.parse(file, \"fasta\"):\n",
    "                if record.id==target_contig:\n",
    "                    target_bin=file.rstrip(\".fa\")\n",
    "    #\n",
    "    \n",
    "    for record in SeqIO.parse(dbfile, \"fasta\"):\n",
    "        if record.id==target_contig:\n",
    "            #contig_sequence_with_putative_protospacer[contigname]=record.seq\n",
    "            for start, end, matched_seq in protospacers_of[target_contig]:\n",
    "                sequence=record.seq\n",
    "                revcomp=False\n",
    "                if start>end:\n",
    "                    print(\"Match on minus, taking reverse complement\")\n",
    "                    s=start\n",
    "                    start=end\n",
    "                    end=s\n",
    "                    revcomp=True\n",
    "                extracted_protospacer=sequence[start-1:end].reverse_complement() if revcomp else sequence[start-1:end]\n",
    "          #      if not extracted_protospacer==matched_seq:\n",
    "           #         print(\"matched seq:\", matched_seq)\n",
    "            #        print(\"extracted:\", extracted_protospacer)\n",
    "             #       raise Exception(\"Extracted sequence differs:\\nExtracted sequence reverse complement?\"+str(revcomp))\n",
    "                print(\"Is extracted putative protospacer equal to Blasted?\",extracted_protospacer==matched_seq)\n",
    "                print(\"original match:\",matched_seq)\n",
    "                print(\"start:\",start,\"end:\",end)\n",
    "                print(\"protospacer extracted:\",extracted_protospacer)\n",
    "                flanking_length=50\n",
    "                print(\"upstream sequence\")\n",
    "                # Warning! Indexing on strings is different than slicing on strings\n",
    "                if start>=(flanking_length+1):\n",
    "                    upstream_start=start-(flanking_length+1)\n",
    "                    added_nucleotides=\"\"\n",
    "                else:\n",
    "                    upstream_start=0\n",
    "                    added_nucleotides=\"X\"*((flanking_length+1)-start)\n",
    "                print(\"TEST: should end with TACC\")\n",
    "                upseq=added_nucleotides+sequence[upstream_start:start-1] #immediately flanking, no overlap\n",
    "                print(\"TEST: should end with TACC\")\n",
    "                \n",
    "                print(\"downstream sequence\")\n",
    "                last_pos=len(sequence)\n",
    "                if last_pos-end>=flanking_length:\n",
    "                    downstream_end=end+flanking_length\n",
    "                    added_nucleotides=\"\"\n",
    "                else: #check it works. probly no need\n",
    "                    downstream_end=last_pos\n",
    "                    added_nucleotides=\"X\"*(flanking_length-(last_pos-end))\n",
    "                downseq=sequence[end:downstream_end]+added_nucleotides  #immediately flanking, no overlap\n",
    "                \n",
    "                # pezzo revcomp\n",
    "                if revcomp:\n",
    "                    temp=downseq\n",
    "                    downseq=upseq.reverse_complement()\n",
    "                    upseq=temp.reverse_complement()\n",
    "\n",
    "                print(\"up\",upseq,len(upseq))\n",
    "                print(\"down:\",downseq,len(downseq))\n",
    "                f.write(target_contig+\",\"+str(upseq)+\",\"+str(downseq)+\",\"+target_sample_file+\",\"+target_bin+\"\\n\")\n",
    "                #todo add if is binned to the thing above\n",
    "                #todo: remove these below\n",
    "                if not target_contig in flanking_sequences_of_putative_protospacers_of.keys():\n",
    "                    flanking_sequences_of_putative_protospacers_of[target_contig]=[]\n",
    "                flanking_sequences_of_putative_protospacers_of[target_contig].append((upseq,downseq))\n",
    "                \n",
    "            print(flanking_sequences_of_putative_protospacers_of)\n",
    "\n",
    "            break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
